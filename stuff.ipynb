{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Using model text-davinci-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">003</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mUsing model text-davinci-\u001b[0m\u001b[1;36m003\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from langchain import LLMChain, OpenAI, PromptTemplate\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from openai.api_resources import engine\n",
    "from pqdm.threads import pqdm\n",
    "from rich.console import Console\n",
    "\n",
    "load_dotenv()\n",
    "MODEL_NAME = \"gpt-3.5-turbo-16k\"\n",
    "console = Console()\n",
    "llm = OpenAI(temperature=0)\n",
    "console.print(\"Using model\", llm.model_name, style=\"bold green\")\n",
    "text_splitter = CharacterTextSplitter()\n",
    "CHUNK_SIZE = 4000\n",
    "OVERLAP = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file: Path):\n",
    "    content = file.read_text()\n",
    "    console.print(\"Processing\", file, style=\"bold yellow\")\n",
    "    console.print(\"Number of characters:\", len(content), style=\"bold green\")\n",
    "\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=OVERLAP,\n",
    "        length_function=len,\n",
    "    )\n",
    "    texts = splitter.split_text(content)\n",
    "    console.print(\"Number of chunks:\", len(texts), style=\"bold green\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(\"paper.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Processing paper.txt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mProcessing paper.txt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Number of characters: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">69701</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mNumber of characters: \u001b[0m\u001b[1;36m69701\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Number of chunks: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mNumber of chunks: \u001b[0m\u001b[1;36m20\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = process_document(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=llm.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-davinci-003', deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=None, openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_search = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = doc_search.similarity_search(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The results, presented in Figure , show that once again GPT3 achieves near-perfect performance on in-distribution, but fails entirely in generalizing to OOD cases---in particular, wider or deeper computation graphs. \\nThese results indicate that even when training directly with guidance on the computation steps, models still fail to learn component operations in a generalizable manner. This observation holds for all tasks (See details in  ).  \\nThese findings suggest that the autoregressive characteristic of Transformers, which forces them to tackle problems sequentially, presents a fundamental challenge that cannot be resolved by instructing the model to generate a step-by-step solution. Instead, models face the fundamental challenge of simply depending on a greedy process of producing the next word to make predictions without a rigorous global understanding of the task. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBreaking Down Successes and Failures of Transformers\\n\\n\\n\\nInformation Gain Explains Where Transformers Partially Excel\\n\\nAt times \\nTransformers predict partially correct answers even when the overall response is incorrect. We speculate that this may be due to particularities in the task distribution that allow for guessing partial answers without performing the full multi-step reasoning that the task requires. \\n\\nUsing relative information gain (defined in theory_inf_gain), we can predict surface patterns that a model is likely to learn and contrast them empirically. \\nFor multiplication, relative information gain shows that the first digit (two digits) of the output highly correlates with the first digit (two digits) of each input number (see appendix:inf_gain_multiplication). Hence, this spurious pattern is likely to be learned by a model. \\nSimilarly, the prediction of the last digit (or two digits) of the output is observed to solely rely on the last digit (or two digits) of each input number. This pattern holds true due to the principles of modulo arithmetic, which ensures the validity of this relationship in all cases.\\nEmpirically, we verify that models indeed learn the patterns we predicted and other patterns as well (e.g., order of magnitude of the answer, number of trailing zeros for multiplication) in all the settings with and without scratchpad. See details for multiplication, plus dynamic programming task analysis in \\nappendix:surface_patterns.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThese experiments suggest that if an output element heavily relies on a single or a small set of input features, Transformers are likely to recognize such correlation during training and directly map these input features to predict the output element in testing, without going through the rigorous multi-hop reasoning and giving a false illusion of performing compositional reasoning.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformers Reduce Multi-Step Compositional Reasoning into Linearized Subgraph Matching\\n\\n\\nWe now explore whether models' correct predictions on unseen test data are due to learning the underlying algorithm or, instead, explainable by exposure to similar training examples.\\nWe hypothesize that, beyond simple memorization, Transformers largely rely on pattern matching for solving these tasks. To test this, we calculate the average frequency with which partial computations needed to solve an instance appear in the training data, for both correctly and wrongly predicted examples.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGiven a model-generated computation graph  we analyze how often the full computation of each node  is seen in training. We define 's full computation as the subgraph induced by all ancestors of  including , denoted . We say that  is seen during training if  for some computation graph  in training, and for some . \\nWe characterize complexity of a full computation subgraph by its depth, as defined in def_computation_graph.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the contribution of this work?\"\n",
    "response = chain.run(input_documents=docs,question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  This work contributes to the understanding of the successes and failures of\n",
      "Transformers in compositional tasks by providing a rigorous empirical and\n",
      "theoretical examination. Specifically, we evaluate the performance of GPT3,\n",
      "ChatGPT, and GPT4 on our suite of compositional tasks, using zero-shot, few-\n",
      "shot, and finetuning techniques. We then analyze the results and identify the\n",
      "challenges that Transformers face in mastering multi-step compositional\n",
      "problems. We additionally provide theoretical insights on the difficulty of\n",
      "generalizing these models and suggest empirical strategies for improving them.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "print(*textwrap.wrap(response,80),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = RetrievalQA(search_type='similarity', search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
